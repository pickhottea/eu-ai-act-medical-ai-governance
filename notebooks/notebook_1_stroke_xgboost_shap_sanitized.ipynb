{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An_7pxJ5yVfe"
      },
      "source": [
        "# Notebook 1 — Stroke Dataset\n",
        "## Bias Analysis & Explainability (XGBoost + SHAP)\n",
        "\n",
        "(Sanitized public version)\n",
        "\n",
        "\n",
        "## Scope & Governance Disclaimer\n",
        "\n",
        "This notebook demonstrates a **high-risk medical tabular AI use case**\n",
        "(stroke risk prediction) with a focus on:\n",
        "\n",
        "- explainability (SHAP),\n",
        "- bias and subgroup reliability analysis,\n",
        "- and human-in-the-loop review policies.\n",
        "\n",
        "It is intended as a **governance- and compliance-oriented demonstration**\n",
        "and does **not** represent a deployed clinical decision system.\n",
        "\n",
        "**Outputs from this notebook are treated as governance evidence artefacts (C2/C3/D1/D3/D4) and are linked in the repository’s evidence structure. No clinical claims are made.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAXMqhcfN2K6"
      },
      "source": [
        "## Environment & Data Access (Sanitized)\n",
        "\n",
        "Environment setup and dataset acquisition steps are intentionally excluded\n",
        "to avoid exposing operational details and credential handling practices.\n",
        "\n",
        "The scope of this notebook is limited to governance-relevant artifacts,\n",
        "including explainability outputs, evaluation evidence, and audit logs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wv_EabhWU9a"
      },
      "source": [
        "### Dataset Provenance & Reproducibility Notes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HeT8jAZPxXd"
      },
      "source": [
        "## Step 1 — Data Preprocessing\n",
        "\n",
        "Perform basic data cleaning and preparation:\n",
        "\n",
        "* The dataset is retrieved from Kaggle using the official Kaggle CLI.\n",
        "* The dataset slug is recorded to support reproducibility.\n",
        "* Results may change if the dataset is updated on Kaggle; therefore, this notebook documents the access method and dataset identifier used at execution time.\n",
        "* Handle missing values (e.g. BMI, smoking status)\n",
        "* Separate features and target label (stroke)\n",
        "* Split data into training and test sets\n",
        "* Encode categorical variables and pass through numerical features\n",
        "\n",
        "### Purpose\n",
        "Prepare data for model training while preserving demographic attributes (age, gender) for subgroup analysis.\n",
        "\n",
        "Preprocessing choices are kept minimal for demonstration and auditability; extensive feature engineering is omitted.\n",
        "\n",
        "### Expected columns\n",
        "age, gender, hypertension, heart_disease, avg_glucose_level, bmi, smoking_status, stroke\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9mePOxTPylx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset\n",
        "# -----------------------------\n",
        "# DATA_PATH intentionally abstracted in the public version.\n",
        "# Expected schema is documented below for reproducibility.\n",
        "DATA_PATH = \"data/stroke/healthcare-dataset-stroke-data.csv\"  # placeholder\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Basic cleaning: missing values\n",
        "# -----------------------------\n",
        "# Fill missing BMI with the median (simple baseline strategy)\n",
        "df[\"bmi\"] = df[\"bmi\"].fillna(df[\"bmi\"].median())\n",
        "\n",
        "# Treat missing smoking status as an explicit category\n",
        "df[\"smoking_status\"] = df[\"smoking_status\"].fillna(\"Unknown\")\n",
        "\n",
        "# -----------------------------\n",
        "# Separate features and target\n",
        "# -----------------------------\n",
        "y = df[\"stroke\"].astype(int)\n",
        "X = df.drop(columns=[\"stroke\", \"id\"], errors=\"ignore\")\n",
        "\n",
        "# -----------------------------\n",
        "# Train/test split (keep demographic fields for later slicing)\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Encoding: identify categorical vs numerical columns\n",
        "# -----------------------------\n",
        "categorical_cols = [c for c in X.columns if X[c].dtype == \"object\"]\n",
        "numerical_cols = [c for c in X.columns if c not in categorical_cols]\n",
        "\n",
        "# One-hot encode categorical features; keep numerical features as-is\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
        "        (\"num\", \"passthrough\", numerical_cols),\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Preprocessing ready.\")\n",
        "print(\"Categorical columns:\", categorical_cols)\n",
        "print(\"Numerical columns:\", numerical_cols)\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhKMBo57O9tJ"
      },
      "source": [
        "## Step 2 — Model Training (XGBoost Classifier)\n",
        "\n",
        "Train an XGBoost classification model using a preprocessing pipeline. Hyperparameter tuning is intentionally omitted; the goal is explainability and governance evidence rather than optimization.\n",
        "\n",
        "### Model Characteristics\n",
        "\n",
        "* Gradient-boosted decision trees\n",
        "* Suitable for tabular medical data\n",
        "* Compatible with SHAP explainability\n",
        "\n",
        "### Purpose\n",
        "Establish a baseline predictive model for stroke risk estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkBaKAwO_N2"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# -----------------------------\n",
        "# Define XGBoost model\n",
        "# -----------------------------\n",
        "xgb_model = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    #Hyperparameters are chosen as stable defaults for reproducibility, not optimized for performance.\n",
        "    reg_lambda=1.0,\n",
        "    random_state=42,\n",
        "    eval_metric=\"logloss\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Build training pipeline\n",
        "# -----------------------------\n",
        "# Pipeline includes preprocessing + model training\n",
        "pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"model\", xgb_model),\n",
        "    ]\n",
        ")\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTszT974PAK5"
      },
      "source": [
        "## Step 3 — Model Performance Evaluation\n",
        "\n",
        "Evaluate model performance on the test set using:\n",
        "\n",
        "* Predicted probabilities\n",
        "* Area Under the ROC Curve (AUC)\n",
        "\n",
        "### Purpose\n",
        "Verify that the model reaches a reasonable performance level before explainability and bias analysis.\n",
        "\n",
        "**Interpretation**\n",
        "Performance metrics (e.g., ROC-AUC) are used as a **sanity check**\n",
        "Reported metrics are sanity checks only and must not be interpreted as clinical utility or deployment readiness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by9oJQxVPCNX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# -----------------------------\n",
        "# Predict probabilities on the test set\n",
        "# -----------------------------\n",
        "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# -----------------------------\n",
        "# Compute AUC\n",
        "# -----------------------------\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Test AUC: {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEMZZVe8xlTy"
      },
      "source": [
        "The model achieves an AUC of ~0.82 on a held-out test set in this demonstration setup.\n",
        "\n",
        "This metric is reported as **baseline evidence** and is **not** used to claim clinical readiness or deployment suitability.\n",
        "The primary focus of this notebook is explainability and governance-oriented review.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaOcvr_NPFeY"
      },
      "source": [
        "## Step 4 — Global Explainability (C2: SHAP Summary)\n",
        "\n",
        "Compute SHAP values for the test set and generate a global SHAP summary plot.\n",
        "\n",
        "### Evidence Produced\n",
        "\n",
        "* Feature importance ranking\n",
        "* Direction and magnitude of feature contributions\n",
        "\n",
        "### Purpose\n",
        "Provide global transparency into which clinical and demographic features most influence model predictions\n",
        "(EU AI Act Art. 13 – Transparency).\n",
        "\n",
        "**High-level Interpretation (Global)**\n",
        "The global SHAP summary indicates that demographic and clinical features\n",
        "(e.g., age, average glucose level, BMI) are the dominant drivers of stroke risk prediction.\n",
        "Higher values of these features tend to increase the predicted stroke risk,\n",
        "which is consistent with domain expectations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYc_2ITyPF_6"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Transform features for SHAP\n",
        "# -----------------------------\n",
        "X_test_transformed = pipeline.named_steps[\"preprocess\"].transform(X_test)\n",
        "feature_names = pipeline.named_steps[\"preprocess\"].get_feature_names_out()\n",
        "\n",
        "# -----------------------------\n",
        "# SHAP TreeExplainer for XGBoost models\n",
        "# -----------------------------\n",
        "explainer = shap.TreeExplainer(pipeline.named_steps[\"model\"])\n",
        "shap_values = explainer.shap_values(X_test_transformed)\n",
        "\n",
        "# -----------------------------\n",
        "# Global SHAP summary plot (C2 evidence)\n",
        "# -----------------------------\n",
        "plt.figure()\n",
        "shap.summary_plot(\n",
        "    shap_values,\n",
        "    X_test_transformed,\n",
        "    feature_names=feature_names,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"C2_stroke_shap_summary\", dpi=200)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: C2_stroke_shap_summary.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMxBfVh_PIAi"
      },
      "source": [
        "## Step 5 — Local Explainability (C3: SHAP Waterfall)\n",
        "\n",
        "Select representative individual cases (e.g. true positive, false positive) and generate SHAP waterfall plots.\n",
        "\n",
        "### Evidence Produced\n",
        "\n",
        "* Case-level explanation of model decisions\n",
        "\n",
        "### Purpose\n",
        "Support human-in-the-loop review by answering\n",
        "“Why did the model produce this output for this specific patient?”\n",
        "(EU AI Act Art. 13 & 14 – Human Oversight).\n",
        "\n",
        "**Human-in-the-Loop Interpretation**\n",
        "This local SHAP explanation shows how individual features contributed\n",
        "to the prediction for a specific patient.\n",
        "Such case-level explanations enable a human reviewer to assess\n",
        "whether the model’s reasoning is plausible before acting on the output.\n",
        "\n",
        "### Compatibility Note (Sanitized Public Version)\n",
        "\n",
        "In some execution environments, SHAP waterfall plots may display\n",
        "inconsistent feature alignment due to preprocessing pipelines\n",
        "(e.g. one-hot encoding) and library version differences.\n",
        "\n",
        "In such cases, the implementation may switch to a simplified\n",
        "`shap.Explanation(...)` representation or alternative visualization\n",
        "to preserve interpretability.\n",
        "\n",
        "These implementation details are intentionally abstracted\n",
        "in this public version, as the focus of this notebook is on\n",
        "governance-relevant explainability evidence rather than\n",
        "visualization mechanics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-7hMmT7PJ53"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# Convert probabilities to binary predictions using a fixed threshold\n",
        "# -----------------------------\n",
        "THRESHOLD = 0.5\n",
        "y_pred = (y_proba >= THRESHOLD).astype(int)\n",
        "\n",
        "# -----------------------------\n",
        "# Select representative cases:\n",
        "# - True Positive (TP): y=1, pred=1\n",
        "# - False Positive (FP): y=0, pred=1\n",
        "# -----------------------------\n",
        "tp_indices = np.where((y_test.values == 1) & (y_pred == 1))[0]\n",
        "fp_indices = np.where((y_test.values == 0) & (y_pred == 1))[0]\n",
        "\n",
        "def plot_waterfall(index, filename):\n",
        "    # Waterfall plot shows case-level feature contributions\n",
        "    shap.plots._waterfall.waterfall_legacy(\n",
        "        explainer.expected_value,\n",
        "        shap_values[index],\n",
        "        feature_names=feature_names,\n",
        "        features=X_test_transformed[index],\n",
        "        show=False\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, dpi=200)\n",
        "    plt.show()\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "if len(tp_indices) > 0:\n",
        "    plot_waterfall(tp_indices[0], \"CC3_stroke_shap_waterfall_TP.png\")\n",
        "else:\n",
        "    print(\"No TP case found under current threshold; consider adjusting threshold or checking class imbalance.\")\n",
        "\n",
        "if len(fp_indices) > 0:\n",
        "    plot_waterfall(fp_indices[0], \"C3_stroke_shap_waterfall_FP.png\")\n",
        "else:\n",
        "    print(\"No FP case found under current threshold; consider adjusting threshold or checking class imbalance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpUIkq4xPKfB"
      },
      "source": [
        "## Step 6 — Bias Analysis (D1: Subgroup Slicing)\n",
        "\n",
        "Conduct subgroup performance analysis using:\n",
        "\n",
        "* Age buckets (e.g. <40, 40–60, >60)\n",
        "* Gender groups\n",
        "\n",
        "Metrics evaluated:\n",
        "\n",
        "* AUC\n",
        "* False Negative Rate (FNR)\n",
        "* False Positive Rate (FPR)\n",
        "\n",
        "### Purpose\n",
        "Identify potential reliability gaps across demographic subgroups\n",
        "(EU AI Act Art. 9, Art. 10, Art. 15).\n",
        "\n",
        "**Interpretation note (governance):**  \n",
        "\n",
        "mall subgroup sample sizes may lead to unstable estimates; results should be treated as screening signals rather than conclusions.\n",
        "\n",
        "This subgroup comparison does **not** prove discrimination.  \n",
        "\n",
        "It serves as an early **reliability signal** that may trigger further review (data quality, representativeness, proxy feature assessment).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzOzhqzVPMm-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "# -----------------------------\n",
        "# Helper: compute subgroup metrics\n",
        "# -----------------------------\n",
        "def compute_group_metrics(mask, label):\n",
        "    y_true = y_test.values[mask]\n",
        "    y_score = y_proba[mask]\n",
        "    y_hat = (y_score >= THRESHOLD).astype(int)\n",
        "\n",
        "    # AUC requires both classes present\n",
        "    auc_val = roc_auc_score(y_true, y_score) if len(np.unique(y_true)) > 1 else np.nan\n",
        "\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat, labels=[0, 1]).ravel()\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
        "\n",
        "    return {\n",
        "        \"group\": label,\n",
        "        \"n_samples\": int(mask.sum()),\n",
        "        \"AUC\": auc_val,\n",
        "        \"FNR\": fnr,\n",
        "        \"FPR\": fpr,\n",
        "    }\n",
        "\n",
        "results = []\n",
        "\n",
        "# -----------------------------\n",
        "# Age buckets\n",
        "# -----------------------------\n",
        "age_bins = pd.cut(\n",
        "    X_test[\"age\"],\n",
        "    bins=[0, 40, 60, 120],\n",
        "    labels=[\"<40\", \"40-60\", \">60\"],\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "for g in [\"<40\", \"40-60\", \">60\"]:\n",
        "    mask = (age_bins == g)\n",
        "    results.append(compute_group_metrics(mask, f\"age:{g}\"))\n",
        "\n",
        "# -----------------------------\n",
        "# Gender groups\n",
        "# -----------------------------\n",
        "gender_series = X_test[\"gender\"].astype(str)\n",
        "for g in sorted(gender_series.unique()):\n",
        "    mask = (gender_series == g)\n",
        "    results.append(compute_group_metrics(mask, f\"gender:{g}\"))\n",
        "\n",
        "bias_df = pd.DataFrame(results)\n",
        "bias_df.to_csv(\"D1_bias_metrics_stroke.csv\", index=False)\n",
        "\n",
        "print(bias_df)\n",
        "print(\"Saved: D1_bias_metrics_stroke.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXYUms4EPNLA"
      },
      "source": [
        "## Step 7 — Bias Visualization\n",
        "\n",
        "Visualize subgroup performance differences using bar charts (e.g. FNR and FPR by subgroup).\n",
        "\n",
        "### Purpose\n",
        "Enable intuitive inspection of bias patterns and support governance discussions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD5no3qZPO14"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# Plot FNR by subgroup\n",
        "# -----------------------------\n",
        "plt.figure()\n",
        "bias_df.set_index(\"group\")[\"FNR\"].plot(kind=\"bar\")\n",
        "plt.title(\"False Negative Rate by Subgroup (Stroke)\")\n",
        "plt.ylabel(\"FNR\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"D1_bias_FNR_stroke.png\", dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved: D1_bias_FNR_stroke.png\")\n",
        "\n",
        "# -----------------------------\n",
        "# Plot FPR by subgroup\n",
        "# -----------------------------\n",
        "plt.figure()\n",
        "bias_df.set_index(\"group\")[\"FPR\"].plot(kind=\"bar\")\n",
        "plt.title(\"False Positive Rate by Subgroup (Stroke)\")\n",
        "plt.ylabel(\"FPR\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"D1_bias_FPR_stroke.png\", dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved: D1_bias_FPR_stroke.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iQZOQsdPPTC"
      },
      "source": [
        "## Step 8 — Human Review Policy Definition (D3)\n",
        "\n",
        "Define a simple human-in-the-loop review policy, including:\n",
        "\n",
        "* Low-confidence prediction thresholds\n",
        "* Monitoring of high-risk subgroups\n",
        "* Override and logging requirements\n",
        "\n",
        "### Purpose\n",
        "Demonstrate operational human oversight mechanisms\n",
        "(EU AI Act Art. 14 – Human Oversight).\n",
        "\n",
        "### Demo policy:\n",
        "\n",
        "If subgroup error ratio exceeds a defined threshold (e.g. > 1.2), trigger a human review:\n",
        "\n",
        "- document the finding,\n",
        "- inspect data representativeness and potential proxy effects,\n",
        "- decide whether mitigation or retraining is required.\n",
        "\n",
        "Note: The formal policy version is maintained under docs/policies/ and this notebook output serves as evidence snapshot for a specific run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqzgt0gIPRRH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Compute global baseline metrics (for comparison)\n",
        "# -----------------------------\n",
        "global_row = compute_group_metrics(np.ones(len(X_test), dtype=bool), \"global\")\n",
        "global_fnr = global_row[\"FNR\"]\n",
        "global_fpr = global_row[\"FPR\"]\n",
        "\n",
        "# -----------------------------\n",
        "# Identify high-risk subgroups based on relative gap\n",
        "# Example rule: subgroup FNR or FPR exceeds global by > 20% relative\n",
        "# -----------------------------\n",
        "RISK_GAP_REL = 0.20\n",
        "\n",
        "high_risk_groups = []\n",
        "for _, r in bias_df.iterrows():\n",
        "    if r[\"group\"] == \"global\":\n",
        "        continue\n",
        "    fnr_gap = (r[\"FNR\"] - global_fnr) / global_fnr if (global_fnr and not np.isnan(global_fnr) and global_fnr > 0) else np.nan\n",
        "    fpr_gap = (r[\"FPR\"] - global_fpr) / global_fpr if (global_fpr and not np.isnan(global_fpr) and global_fpr > 0) else np.nan\n",
        "    if (not np.isnan(fnr_gap) and fnr_gap > RISK_GAP_REL) or (not np.isnan(fpr_gap) and fpr_gap > RISK_GAP_REL):\n",
        "        high_risk_groups.append(r[\"group\"])\n",
        "\n",
        "# -----------------------------\n",
        "# Define low-confidence band for mandatory review\n",
        "# -----------------------------\n",
        "LOW_CONF_LO = 0.40\n",
        "LOW_CONF_HI = 0.60\n",
        "\n",
        "policy_md = f\"\"\"# D3_human_review_policy.md\n",
        "\n",
        "## Intended Use\n",
        "This model provides decision support for stroke risk prediction. Final clinical decisions remain with healthcare professionals.\n",
        "\n",
        "## Review Triggers (Human-in-the-loop)\n",
        "### 1) Low-confidence predictions (mandatory review)\n",
        "- If predicted risk is between **{LOW_CONF_LO:.2f} and {LOW_CONF_HI:.2f}**, a human reviewer must validate the case.\n",
        "\n",
        "### 2) High-risk subgroup monitoring (heightened oversight)\n",
        "- The following subgroups showed elevated error risk compared to global baseline (relative gap threshold: {RISK_GAP_REL:.0%}):\n",
        "{chr(10).join([f\"- {g}\" for g in high_risk_groups]) if high_risk_groups else \"- (No subgroup exceeded the predefined risk gap threshold in this run.)\"}\n",
        "\n",
        "### 3) Override and logging requirements\n",
        "- A human reviewer may override the model output.\n",
        "- Each override must be recorded with:\n",
        "  - reviewer role / ID (pseudonymous)\n",
        "  - timestamp\n",
        "  - model version / run ID\n",
        "  - original prediction + confidence\n",
        "  - final decision\n",
        "  - short rationale (free text)\n",
        "- Logs must not contain direct personal identifiers.\n",
        "\n",
        "## Operational Notes\n",
        "- Recalibration or retraining should be considered if persistent subgroup gaps are observed.\n",
        "- Incident handling should be triggered if performance drops below defined safety thresholds.\n",
        "\n",
        "## Evidence Links\n",
        "- Bias metrics: `D1_bias_metrics_stroke.csv`\n",
        "- Bias plots: `D1_bias_FNR_stroke.png`, `D1_bias_FPR_stroke.png`\n",
        "- Explainability: `C2_shap_summary_stroke.png`, local waterfalls in `C3_*`\n",
        "\"\"\"\n",
        "\n",
        "with open(\"D3_human_review_policy.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(policy_md)\n",
        "\n",
        "print(\"Saved: D3_human_review_policy.md\")\n",
        "print(\"High-risk subgroups flagged:\", high_risk_groups)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QDCK0yTEaBe"
      },
      "source": [
        "## Step 9 — Record-Keeping & Audit Trail (Art. 12 Spirit)\n",
        "\n",
        "For each run, a machine-readable audit log is generated, capturing:\n",
        "- Dataset and split identifiers\n",
        "- Model configuration\n",
        "- Evaluation metrics\n",
        "- Explainability artefacts\n",
        "- Bias/robustness signals\n",
        "- Review policy outcomes\n",
        "\n",
        "**Purpose**  \n",
        "Provide an auditable, traceable evidence trail that bridges\n",
        "a notebook-based demo and regulatory inspection requirements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timezone\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import json, uuid, os\n",
        "\n",
        "# -----------------------------\n",
        "# Prepare audit directory\n",
        "# -----------------------------\n",
        "os.makedirs(\"audit\", exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Run identifiers\n",
        "# -----------------------------\n",
        "run_id = str(uuid.uuid4())\n",
        "timestamp = datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "# -----------------------------\n",
        "# Basic performance snapshot\n",
        "# -----------------------------\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred).tolist()\n",
        "\n",
        "# -----------------------------\n",
        "# Audit record (Art. 12 spirit)\n",
        "# -----------------------------\n",
        "audit_record = {\n",
        "    \"run_id\": run_id,\n",
        "    \"timestamp\": timestamp,\n",
        "    \"project\": \"EU AI Act Governance Demo\",\n",
        "    \"project_id\": \"Project-1-Stroke-XGBoost\",\n",
        "    \"model\": {\n",
        "        \"type\": \"XGBoost\",\n",
        "        \"task\": \"stroke risk prediction (tabular)\",\n",
        "        \"explainability_method\": \"SHAP\"\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"source\": \"Kaggle (public benchmark)\",\n",
        "        \"dataset_type\": \"tabular clinical-style data\",\n",
        "        \"split\": \"test\"\n",
        "    },\n",
        "    \"performance_snapshot\": {\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"confusion_matrix\": cm\n",
        "    },\n",
        "    \"explainability_artifacts\": [\n",
        "        \"C2_stroke_shap_summary.png\",\n",
        "        \"C3_stroke_shap_waterfall_TP.png\",\n",
        "        \"C3_stroke_shap_waterfall_FP.png\"\n",
        "    ],\n",
        "    \"bias_artifacts\": [\n",
        "        \"D1_bias_metrics_stroke.csv\",\n",
        "        \"D1_bias_FNR_stroke.png\",\n",
        "        \"D1_bias_FPR_stroke.png\"\n",
        "    ],\n",
        "    \"human_oversight\": {\n",
        "        \"review_required\": True,\n",
        "        \"trigger\": \"False positive example included for explainability review\",\n",
        "        \"policy_reference\": \"P4_human_oversight_review_triggers.md\"\n",
        "    },\n",
        "    \"notes\": (\n",
        "        \"This audit record is a demonstration artifact generated \"\n",
        "        \"for governance and compliance illustration purposes. \"\n",
        "        \"It does not represent a production deployment log.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# Write append-only audit log\n",
        "# -----------------------------\n",
        "with open(\"audit/audit_log.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write(json.dumps(audit_record) + \"\\n\")\n",
        "\n",
        "print(\"Audit record written:\", run_id)\n"
      ],
      "metadata": {
        "id": "ZAl6ZfITMKC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQHR_w6GPTA4"
      },
      "source": [
        "## Summary of Evidence Generated\n",
        "\n",
        "This notebook produces the following compliance-relevant artefacts:\n",
        "- **C2**: Global explainability (SHAP summary)\n",
        "- **C3**: Local explainability (SHAP waterfall plots)\n",
        "- **D1**: Subgroup reliability analysis (age, gender)\n",
        "- **D3**: Human-in-the-loop review policy\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}