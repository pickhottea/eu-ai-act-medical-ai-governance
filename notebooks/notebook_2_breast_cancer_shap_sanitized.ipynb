{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 2 — Breast Cancer Wisconsin\n",
        "## Explainability Demo (Global + Local SHAP)\n",
        "\n",
        "## Scope & Disclaimer\n",
        "\n",
        "This notebook demonstrates SHAP-based explainability on a clean, well-known medical classification dataset (Breast Cancer Wisconsin).\n",
        "\n",
        "It is intended as a **method demonstration** for explainability (**global + local**) and does **not** constitute clinical performance or deployment claims.\n",
        "\n",
        "Outputs from this notebook are treated as explainability evidence artefacts (C2/C3)\n",
        "to support governance review and auditability.\n",
        "No clinical performance or deployment claims are made.\n",
        "\n",
        "## Key Goal\n",
        "Produce audit-friendly explainability artefacts:\n",
        "- **C2**: Global SHAP summary (feature importance + direction)\n",
        "- **C3**: Local SHAP waterfall (case-level explanation)\n",
        "\n"
      ],
      "metadata": {
        "id": "DRKg1Por3xiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment & Data Access (Sanitized)\n",
        "\n",
        "To keep the notebook concise and review-focused, environment setup\n",
        "and data download steps are not included.\n",
        "\n",
        "The notebook assumes that required data and dependencies are\n",
        "available in the execution environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "QCiwlfv430NC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 — Dataset Loading (Breast Cancer Wisconsin)\n",
        "\n",
        "Load the Breast Cancer Wisconsin dataset from scikit-learn.\n",
        "\n",
        "**Purpose**\n",
        "Use a clean, well-known medical dataset to demonstrate explainability methods in a stable “best-case” scenario.\n"
      ],
      "metadata": {
        "id": "Zf8DRdtE3zwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1 — Dataset Loading ===\n",
        "\n",
        "# -----------------------------\n",
        "# Load dataset from sklearn\n",
        "# -----------------------------\n",
        "data = load_breast_cancer()\n",
        "# NOTE: sklearn breast cancer target: 0 = malignant, 1 = benign\n",
        "\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "# sklearn convention: 0 = malignant, 1 = benign\n",
        "\n",
        "label_map = {0: \"malignant\", 1: \"benign\"}\n",
        "y_label = y.map(label_map)\n",
        "\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"Class distribution:\\n\", y_label.value_counts())\n",
        "display(X.head())\n"
      ],
      "metadata": {
        "id": "AvGlIDKA3-tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — Train/Test Split\n",
        "\n",
        "Split the dataset into train and test sets (stratified).\n",
        "\n",
        "**Purpose**\n",
        "Ensure a fair and reproducible evaluation split while preserving class distribution.\n"
      ],
      "metadata": {
        "id": "7ocTNm3j4ExX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 2 — Train/Test Split ===\n",
        "\n",
        "# -----------------------------\n",
        "# Split data into train/test\n",
        "# -----------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "print(\"Train class ratio:\", y_train.value_counts(normalize=True).to_dict())\n",
        "print(\"Test class ratio:\", y_test.value_counts(normalize=True).to_dict())\n"
      ],
      "metadata": {
        "id": "ifdan1Li4ADm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — Minimal Data Preprocessing (Stability-Oriented)\n",
        "\n",
        "Apply a minimal preprocessing setup to ensure numerical stability\n",
        "and reproducibility, without introducing complex feature engineering.\n",
        "\n",
        "Preprocessing choices are intentionally limited to:\n",
        "- Simple imputation for missing values (if present)\n",
        "- Standardization of numeric features\n",
        "\n",
        "**Purpose**  \n",
        "Provide stable and well-conditioned inputs for the model while keeping\n",
        "the preprocessing pipeline transparent, auditable, and easy to review. This minimal setup reduces hidden transformations and supports audit review.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m8CVnbC_4KL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 3 — Data Preprocessing ===\n",
        "\n",
        "# -----------------------------\n",
        "# Define preprocessing pipeline\n",
        "# -----------------------------\n",
        "preprocess = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n"
      ],
      "metadata": {
        "id": "jmNLVffB4AS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — Model Training (Logistic Regression)\n",
        "\n",
        "Train a Logistic Regression classifier using the preprocessing pipeline.\n",
        "\n",
        "**Purpose**\n",
        "Use an interpretable baseline model that works well on tabular data and is compatible with SHAP.\n"
      ],
      "metadata": {
        "id": "fVmq23xV4N14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 4 — Model Training ===\n",
        "\n",
        "# -----------------------------\n",
        "# Define model\n",
        "# -----------------------------\n",
        "clf = LogisticRegression(max_iter=5000, random_state=42, class_weight=\"balanced\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Build end-to-end pipeline\n",
        "# -----------------------------\n",
        "model = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", clf)\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Train model\n",
        "# -----------------------------\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training done.\")\n"
      ],
      "metadata": {
        "id": "gPO671cr4AhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Model Performance Evaluation\n",
        "\n",
        "Evaluate the trained model on a held-out test set\n",
        "and report baseline classification metrics.\n",
        "\n",
        "Reported metrics are provided as **descriptive evidence**\n",
        "to contextualize explainability results.\n",
        "They are not used to claim clinical suitability\n",
        "or deployment readiness.\n",
        "\n",
        "**Purpose**  \n",
        "Establish a transparent performance reference\n",
        "to support later explainability and audit review.\n",
        "\n",
        "Reported metrics serve as contextual evidence only and must not be interpreted\n",
        "as indicators of clinical utility or deployment readiness.\n",
        "\n"
      ],
      "metadata": {
        "id": "EDhtbK1c4RAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 5 — Model Performance Evaluation ===\n",
        "\n",
        "# -----------------------------\n",
        "# Predict probabilities on the test set\n",
        "# -----------------------------\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # probability for class 1 (benign)\n",
        "\n",
        "# -----------------------------\n",
        "# ROC-AUC\n",
        "# -----------------------------\n",
        "auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"Test ROC-AUC: {auc:.4f}\")\n",
        "\n",
        "# -----------------------------\n",
        "# ROC curve plot\n",
        "# -----------------------------\n",
        "# NOTE: Here ROC treats label=1 (benign) as the positive class,\n",
        "# because we use predict_proba[:, 1].\n",
        "\n",
        "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
        "plt.title(\"ROC Curve (Test Set)\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# Confusion matrix (default threshold 0.5 for benign)\n",
        "# -----------------------------\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# -----------------------------\n",
        "# Classification report\n",
        "# -----------------------------\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"malignant(0)\", \"benign(1)\"]))\n"
      ],
      "metadata": {
        "id": "MMd6_LNj4ApY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUC is reported as a baseline metric in this demonstration setup.\n",
        "\n",
        "This result is provided as **evidence** and is **not** used to claim clinical readiness or deployment suitability.\n"
      ],
      "metadata": {
        "id": "rokHYLRB6Qeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 — Global Explainability (C2: SHAP Summary)\n",
        "\n",
        "Compute SHAP values on the test set and generate a global SHAP summary plot.\n",
        "\n",
        "**Evidence Produced**\n",
        "- Feature importance ranking\n",
        "- Direction and magnitude of feature contributions\n",
        "\n",
        "**Purpose**\n",
        "Provide global transparency into which features most influence model predictions (audit-friendly evidence).\n"
      ],
      "metadata": {
        "id": "pcbcxV3X4Vqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 6 — Global Explainability (C2) ===\n",
        "\n",
        "# -----------------------------\n",
        "# Transform data using the same preprocessing steps\n",
        "# (SHAP needs the final numeric feature space)\n",
        "# -----------------------------\n",
        "fitted_preprocess = model.named_steps[\"preprocess\"]\n",
        "trained_clf = model.named_steps[\"clf\"]\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "X_train_proc = fitted_preprocess.transform(X_train)\n",
        "X_test_proc  = fitted_preprocess.transform(X_test)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Build SHAP explainer for linear model\n",
        "# -----------------------------\n",
        "explainer = shap.Explainer(trained_clf, X_train_proc, feature_names=feature_names)\n",
        "shap_values = explainer(X_test_proc)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Compute SHAP values for test set\n",
        "# -----------------------------\n",
        "shap_values = explainer(X_test_proc)\n",
        "\n",
        "# -----------------------------\n",
        "# Global SHAP summary plot (beeswarm)\n",
        "# -----------------------------\n",
        "shap.summary_plot(shap_values, features=X_test_proc, feature_names=feature_names, show=False)\n",
        "# Positive SHAP values push the prediction towards class 1 (benign)\n",
        "plt.title(\"C2 — Global SHAP Summary (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p_jAoARn4Au4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7 — Local Explainability (C3: SHAP Waterfall)\n",
        "\n",
        "Select representative individual cases (e.g., high-confidence malignant, or a misclassified case)\n",
        "and generate SHAP waterfall plots.\n",
        "\n",
        "**Evidence Produced**\n",
        "- Case-level explanation of model decisions\n",
        "\n",
        "**Purpose**\n",
        "Support human-in-the-loop review by answering:\n",
        "“Why did the model produce this output for this specific case?”\n"
      ],
      "metadata": {
        "id": "6Y--QumR4fTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 7 — Local Explainability (C3) ===\n",
        "\n",
        "# -----------------------------\n",
        "# Select a representative case:\n",
        "# Option A: pick the most \"malignant-like\" case (lowest benign probability)\n",
        "# -----------------------------\n",
        "idx = int(np.argmin(y_proba))\n",
        "print(\"Selected test index:\", idx)\n",
        "print(\"True label:\", label_map[int(y_test.iloc[idx])])\n",
        "print(\"Predicted benign probability:\", float(y_proba[idx]))\n",
        "\n",
        "# -----------------------------\n",
        "# Build local explanation\n",
        "# -----------------------------\n",
        "local_exp = shap_values[idx]\n",
        "\n",
        "# Waterfall plot (top features)\n",
        "shap.plots.waterfall(local_exp, max_display=12, show=False)\n",
        "plt.gca().set_title(\"C3 — Local SHAP Waterfall (Selected Case)\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wlBWPHL94UoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8 (Optional) — Reliability Signal via Simple Slicing\n",
        "\n",
        "This is an optional illustration of group-wise error differences and does not represent a fairness conclusion.\n",
        "This dataset does not contain protected attributes (e.g., sex, race, age), so we do **not** perform a fairness assessment.\n",
        "Instead, we demonstrate a minimal **reliability slicing** approach:\n",
        "\n",
        "- Choose a proxy feature (e.g., `mean radius`)\n",
        "- Split the test set into two groups (low vs high, based on median)\n",
        "- Compare performance (AUC) across slices\n",
        "\n",
        "**Purpose**\n",
        "Provide a governance-style early warning signal: if performance differs strongly across slices, trigger review.\n"
      ],
      "metadata": {
        "id": "1c9b0GDF4k2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 8 — Optional Proxy Slicing ===\n",
        "\n",
        "# -----------------------------\n",
        "# Proxy subgroup split by median of \"mean radius\"\n",
        "# -----------------------------\n",
        "proxy_feature = \"mean radius\"\n",
        "median_val = X_test[proxy_feature].median()\n",
        "\n",
        "group_low = X_test[proxy_feature] < median_val\n",
        "group_high = ~group_low\n",
        "\n",
        "def group_auc(mask):\n",
        "    return roc_auc_score(y_test[mask], y_proba[mask])\n",
        "\n",
        "auc_low = group_auc(group_low)\n",
        "auc_high = group_auc(group_high)\n",
        "ratio = max(auc_low, auc_high) / min(auc_low, auc_high)\n",
        "gap = abs(auc_high - auc_low)\n",
        "\n",
        "print(f\"Proxy split feature: {proxy_feature}\")\n",
        "print(f\"AUC (low):  {auc_low:.4f}\")\n",
        "print(f\"AUC (high): {auc_high:.4f}\")\n",
        "print(f\"Ratio (max/min): {ratio:.3f}\")\n",
        "print(f\"Gap: {gap:.4f}\")\n"
      ],
      "metadata": {
        "id": "cG5WJ5M24U3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9 (Optional) — Review Trigger (Demo Policy)\n",
        "\n",
        "This policy is included for portfolio completeness and mirrors the governance pattern used in Notebook 1.\n",
        "\n",
        "Define a simple human-in-the-loop review policy:\n",
        "* When triggered → log + manual review + document decision.\n",
        "* Flag low-confidence predictions for manual review\n",
        "* Track proxy subgroup gaps as a review trigger\n",
        "\n",
        "**Purpose**\n",
        "Demonstrate an operational oversight mechanism (review triggers + thresholds)."
      ],
      "metadata": {
        "id": "eWI7HReb4qpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 9 — Human Review Policy (Demo) ===\n",
        "\n",
        "# -----------------------------\n",
        "# Low-confidence rule (demo):\n",
        "# Review if predicted benign probability is between [0.40, 0.60]\n",
        "# -----------------------------\n",
        "low_conf_mask = (y_proba >= 0.40) & (y_proba <= 0.60)\n",
        "low_conf_rate = low_conf_mask.mean()\n",
        "review_indices = X_test.index[low_conf_mask].tolist()\n",
        "\n",
        "\n",
        "print(f\"Low-confidence review rate: {low_conf_rate:.3f}\")\n",
        "print(\"Example review indices (first 10):\", review_indices[:10])\n",
        "print(\"Example review probs (first 10):\", y_proba[low_conf_mask][:10])\n",
        "\n",
        "# -----------------------------\n",
        "# Proxy gap review rule (demo):\n",
        "# If AUC gap > 0.05 => trigger review\n",
        "# -----------------------------\n",
        "proxy_gap = abs(auc_high - auc_low)\n",
        "review_trigger = proxy_gap > 0.05\n",
        "\n",
        "print(f\"Proxy AUC gap: {proxy_gap:.4f}\")\n",
        "print(\"Review trigger:\", review_trigger)\n"
      ],
      "metadata": {
        "id": "IZ9AdXOe4U77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10 — Record-Keeping & Audit Trail (Art. 12 Spirit)\n",
        "\n",
        "For each run, a machine-readable audit log is generated, capturing:\n",
        "- Dataset and split identifiers\n",
        "- Model configuration\n",
        "- Evaluation metrics\n",
        "- Explainability artefacts\n",
        "- Bias/robustness signals\n",
        "- Review policy outcomes\n",
        "\n",
        "**Purpose**  \n",
        "Provide an auditable, traceable evidence trail that bridges\n",
        "a notebook-based demo and regulatory inspection requirements."
      ],
      "metadata": {
        "id": "ltUwpmPIFhv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audit logging logic is shown in abstracted form.\n",
        "\n",
        "Exact variable bindings are omitted in this sanitized public version, as data acquisition and execution context are intentionally excluded.\n"
      ],
      "metadata": {
        "id": "EMG4AdC9Dzzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudo-code (sanitized)\n",
        "# In the full execution environment, audit records would capture\n",
        "# run metadata, evaluation metrics, and explainability artefacts.\n",
        "\n",
        "# implementation detail intentionally abstracted\n"
      ],
      "metadata": {
        "id": "t9UTzbH1DxNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary — Evidence Generated\n",
        "\n",
        "### Explainability Evidence\n",
        "- ✅ C2: Global SHAP summary\n",
        "- ✅ C3: Local SHAP waterfall\n",
        "\n",
        "### (Optional) Reliability / Governance Signals\n",
        "- ◻️ Simple slicing or proxy comparison\n",
        "- ◻️ Review trigger logic (demo-level)\n",
        "\n",
        "### Record-Keeping\n",
        "- ✅ Audit log structure demonstrated (see Notebook 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "H68IJyaN4svQ"
      }
    }
  ]
}